base_model: /leonardo/home/userexternal/amohamed/cpt-ift-experiments/src/cpt/gemma2-9b
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

trust_remote_code: true
load_in_8bit: false
load_in_4bit: false
strict: false

device_map: null

pretraining_dataset:
  - path: json
    # name: colossal-oscar #euscrawl-v1.1
    # split: train
    data_files:
      - /leonardo/home/userexternal/amohamed/cpt-ift-experiments/data/euscrawl-v1.1/train.jsonl
      - /leonardo/home/userexternal/amohamed/cpt-ift-experiments/data/euscrawl-v1.1/valid.jsonl
    type: completion
    text_column: text
    # trust_remote_code: true

    # conversation: llama3
    # field: conversations
    # roles:
    #   input:
    #     - system
    #     - gpt
    #   output:
    #     - human

shuffle_merged_datasets: true
# dataset_exact_deduplication: true

test_datasets:
  - path: json
    # name: colossal-oscar #euscrawl-v1.1
    # split: train
    data_files:
      - /leonardo/home/userexternal/amohamed/cpt-ift-experiments/data/euscrawl-v1.1/test.jsonl
  # - path: HiTZ/latxa-corpus-v1.1
  #   name: euscrawl-v1.1
  #   split: test
    type: completion
    text_column: text
    trust_remote_code: true

# chat_template: tokenizer_default_fallback_llama3
hf_use_auth_token: true
# val_set_size: 0.075

output_dir: /leonardo_scratch/large/userexternal/amohamed/gemma2-9b-cpt #/gscratch5/users/asalem/cpt-ift/model/cpt-gemma2-9b

adapter: 
lora_model_dir:

# lora_r: 32
# lora_alpha: 8
# lora_dropout: 0
# lora_target_modules:  ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
# lora_target_linear: true
# lora_fan_in_fan_out:
# lora_on_cpu: true  

sequence_len: 8192
sample_packing: true
eval_sample_packing: false
pad_to_sequence_len: false

#tokens:
#  - "<tool_call>"
#  - "<tool_response>"
#  - "<tools>"
#  - "</tool_call>"
#  - "</tool_response>"
#  - "</tools>"
#  - "<reserved1>"
#  - "<reserved2>"

special_tokens:
   pad_token: <|end_of_text|>

neftune_noise_alpha: 5

wandb_project: cpt-instruction-exp
wandb_entity: ahelhady511
wandb_watch:
wandb_name: cpt_gemma2-9b_full
wandb_log_model: 

gradient_accumulation_steps: 64
micro_batch_size: 1
eval_batch_size: 1
max_steps: 10000
optimizer: adamw_torch
lr_scheduler: cosine
learning_rate: 1e-05


train_on_inputs: false
group_by_length: false
bf16: true
fp16: false
tf32: false

gradient_checkpointing: false
early_stopping_patience:
resume_from_checkpoint:
local_rank:
logging_steps: 1
xformers_attention: false
flash_attention: false

warmup_ratio: 0.03
evals_per_epoch: 4
eval_table_size:
save_strategy:
save_steps: 500
save_total_limit: 5
debug:
deepspeed: 
#/gscratch5/users/asalem/cpt-ift/src/configs/deepspeed/deepspeed_zero3.json
weight_decay: 0.0
fsdp:
  - full_shard
  - auto_wrap
fsdp_config:
  fsdp_offload_params: true
  # fsdp_offload_params: true
# fsdp:
#   - full_shard
#   - auto_wrap
# fsdp_config:
#   fsdp_offload_params: true


# main_process_port: 29501
seed: 33